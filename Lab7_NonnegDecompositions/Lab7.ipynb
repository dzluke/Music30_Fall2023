{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0471ff49-e9b5-4895-b1ed-3cd72de33e75",
   "metadata": {},
   "source": [
    "# Lab 7: Non-negative decompositions for music processing\n",
    "\n",
    "Guest lab by J.J. Burred. jjburred@berkeley.edu\n",
    "\n",
    "In this first part of the lab, we will experiment with decomposing sound spectrograms with **Principal Component Analysis (PCA)** and **Non-Negative Matrix Factorization (NMF)**, and we will create sound layers and synthesize cross-components via **Wiener filtering**.\n",
    "\n",
    "First, run this preliminary code to import the main utilities and load samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee5306-dcb1-4705-bdaa-c5042199c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display, clear_output\n",
    "\n",
    "SAMPLES_PATH = Path('./samples')\n",
    "\n",
    "sample_list = [str(file.name) for file in Path('./samples').iterdir() if file.is_file()]\n",
    "\n",
    "sample_dropdown = widgets.Dropdown(\n",
    "    options=sample_list,\n",
    "    description=\"Sample:\"\n",
    ")\n",
    "\n",
    "# Create a button widget\n",
    "button = widgets.Button(description=\"Load\")\n",
    "\n",
    "# Create an Output widget to display the generated music\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Define a function to be called when the button is clicked\n",
    "def on_button_click(b):\n",
    "    with output_widget:\n",
    "        global y, sr\n",
    "        clear_output(wait=True)  # Clear the output widget without clearing the dropdowns\n",
    "        path = Path('./samples') / sample_dropdown.value\n",
    "        y, sr = librosa.load(path, mono=True, sr=None)\n",
    "        plt.figure(figsize=(10,2))\n",
    "        plt.plot(y)\n",
    "        plt.show()\n",
    "        display(Audio(y, rate=sr))\n",
    "\n",
    "# Attach the function to the button's click event\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the widgets and button\n",
    "widgets.VBox([sample_dropdown, button, output_widget])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3a96a-fee9-48ea-8b49-220996653693",
   "metadata": {},
   "source": [
    "## Section 1: Principal Component Analysis of spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4261fb-d196-4a61-ac54-cda75f758b42",
   "metadata": {},
   "source": [
    "We will first compute and display the magnitude spectrogram of the loaded sound.\n",
    "\n",
    "Start by loading the `toy_target.wav` sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674ce88-3384-4102-a920-3fb12d69fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = librosa.stft(y)      # short-time Fourier transform\n",
    "mag_spec = np.abs(spec)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(mag_spec, ref=np.max), x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b0b9b-c73b-42ec-a5d2-278a4bd2a4fb",
   "metadata": {},
   "source": [
    "Now that we have a real-valued input matrix, let's perform a PCA decomposition on it. We will use the `decomposition` library from `scikit-learn`. We need to previously set the desired number of components for the reconstruction. \n",
    "\n",
    "Scikit-learn uses the same pipeline for all machine larning models (factorizations, clusterings, neural networks...):\n",
    "\n",
    "1. Declare the model with the corresponding parameters.\n",
    "2. Fit the model to the data (a.k.a learning) with the `fit` method.\n",
    "3. If desired, transform the model with the data with the `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68221cf0-49cc-4757-88c1-a496ffc4b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "# declare the model\n",
    "model = PCA(n_components=n_components, svd_solver='full')\n",
    "\n",
    "# learn (= fit model to data)\n",
    "model.fit(mag_spec)\n",
    "\n",
    "# transform data with the model\n",
    "bases = model.transform(mag_spec)\n",
    "\n",
    "# retrieve the activations\n",
    "acts = model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0fc0e-93e7-42ec-853f-b571b633cb2e",
   "metadata": {},
   "source": [
    "Alternatively, in most models it is possible to combine learning and transformation into a single method call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be67af4-3959-435b-9369-11b3706a2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = model.fit_transform(mag_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c1840-adb2-4f54-ad27-46fc4601ea4d",
   "metadata": {},
   "source": [
    "Here is some code to plot the bases and activations aligned with the spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a95fa4-3cff-48a9-a68b-ed1ae5aa621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components(in_matrix, bases, acts, n_components=3):\n",
    "\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    outer_grid = gridspec.GridSpec(2,2, wspace=0.2, hspace=0.2)\n",
    "    ax_outer = []\n",
    "    ax_outer.append(fig.add_subplot(outer_grid[0]))\n",
    "    inner_grid_act = gridspec.GridSpecFromSubplotSpec(1, n_components, subplot_spec=outer_grid[1], wspace=0.2, hspace=0.2)\n",
    "    ax_base = []\n",
    "    for i in range(n_components):\n",
    "        ax_base.append(fig.add_subplot(inner_grid_act[i]))\n",
    "    inner_grid_base = gridspec.GridSpecFromSubplotSpec(n_components, 1, subplot_spec=outer_grid[2], wspace=0.2, hspace=0.2)\n",
    "    ax_act = []\n",
    "    for i in range(n_components):\n",
    "        ax_act.append(fig.add_subplot(inner_grid_base[i]))\n",
    "    \n",
    "    ax_outer[0].clear()\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(in_matrix, ref=np.max), x_axis='time', y_axis='linear', ax = ax_outer[0])\n",
    "    \n",
    "    for c in range(n_components):\n",
    "        ax_act[c].plot(acts[c,:])\n",
    "        ax_act[c].set_ylabel(str(c+1))\n",
    "        ax_act[c].xaxis.set_tick_params(labelbottom=False)\n",
    "        ax_act[c].yaxis.set_tick_params(labelleft=False)\n",
    "        ax_base[c].plot(-bases[:,c],np.arange(bases.shape[0]))\n",
    "        ax_base[c].set_xlabel(str(c+1))\n",
    "        ax_base[c].xaxis.set_tick_params(labelbottom=False)\n",
    "        ax_base[c].yaxis.set_tick_params(labelleft=False)\n",
    "\n",
    "display_components(mag_spec, bases, acts, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e6cdf-39d3-4d8e-8c40-43131c45c728",
   "metadata": {},
   "source": [
    "## Section 2: Non-negative Matrix Factorization of spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902f0b9-0743-4b24-bb8b-516dda8b1ffe",
   "metadata": {},
   "source": [
    "Let's redo a spectrogram decomposition, but this time with NMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68416b00-760e-42b0-92e5-14c4bd4b2bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=n_components, init='random', solver='mu')\n",
    "bases = model.fit_transform(mag_spec)\n",
    "acts = model.components_\n",
    "\n",
    "display_components(mag_spec, bases, acts, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ceca-b41c-44d1-86fd-d306d1591af1",
   "metadata": {},
   "source": [
    "Is it any better than PCA, in terms of interpretable temporal/spectral information from the components? Hopefully yes, but it seems the white noise bursts are not captured very well. Why?\n",
    "\n",
    "By default, the scikit-learn implementation of NMF uses the **Frobenius norm** as cost function. The Frobenius norm is the matrix equivalent of the Euclidean distance (related to the Mean Squared Error), which is not very perceptually relevant. On the other hand, the **Kullback-Leibler (KL) divergence** encourages sparsity of the components (spectra with individual harmonics/partials) and is more robust to noise.\n",
    "\n",
    "So, let's try the KL divergence instead, and perform the decomposition again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d6b1e-be01-43c6-a13d-f1665fa6abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_loss='kullback-leibler'\n",
    "\n",
    "model = NMF(n_components=n_components, init='random', solver='mu', beta_loss=beta_loss)\n",
    "bases = model.fit_transform(mag_spec)\n",
    "acts = model.components_\n",
    "display_components(mag_spec, bases, acts, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d263d-2bfc-484f-8af2-4c9ff6181fb1",
   "metadata": {},
   "source": [
    "Now we're talking!\n",
    "\n",
    "If you run the previous cell several times in a row, you'll notice that the order of the components changes. This is a feature (and often a problem) of NMF. Contrary to PCA, the output components are not sorted in any particular order, and their position change due to the random initilization. In matrix factorization this is known as the **permutation problem**. There are special models that try to avoid it, or alternatively one can sort the components a posteriori, for example via feature extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d8077-bbfb-44e9-b17e-30b04733339c",
   "metadata": {},
   "source": [
    "A third popular cost function in NMF, specially in audio, is the **Itakura-Saito (IS) divergence**, which gives the same importance to lower-energy parts of the spectrogram like higher frequencies or transients/attacks, which are as perceptually important as higher energy parts such as sinusoids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31a521-1f4e-4edb-b12e-b4f5712c7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_loss='itakura-saito'\n",
    "\n",
    "model = NMF(n_components=n_components, init='random', solver='mu', beta_loss=beta_loss)\n",
    "bases = model.fit_transform(mag_spec)\n",
    "acts = model.components_\n",
    "display_components(mag_spec, bases, acts, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee358f-3f83-44a5-b8fe-db98ee2185b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "What's happening under the hood is actually pretty simple. We saw in the lecture that the trick of using adaptive learning rates to enforce non-negativity produces a set of 2 **multiplicative update (MU) rules** for block-coordinate descent (in this case, corresponding to the KL divergence):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} \\circ \\frac{\\frac{\\mathbf{X}}{\\mathbf{WH}}\\mathbf{H}^T}{\\mathbf{1H}^T}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{H} \\leftarrow \\mathbf{H} \\circ \\frac{\\mathbf{W}^T\\frac{\\mathbf{X}}{\\mathbf{WH}}}{\\mathbf{W}^T\\mathbf{1}}\n",
    "\\end{equation}\n",
    "\n",
    "The $\\circ$ symbols means the Hadamard product (element-by-element product), and the division is also element-by-element. $T$ means transpose (flip around the diagonal). The $\\mathbf{1}$ symbols denote a unit diagonal matrix (with only ones at its diagonal). So these rules, together with a random initilization of the matrices, translate to the following lines of Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b143fed-d8cd-4413-b71c-b654cf1f56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_nmf(X, k=10, it=100):\n",
    "\n",
    "    nr = X.shape[0]\n",
    "    nc = X.shape[1]\n",
    "\n",
    "    # random init\n",
    "    H = np.random.randn(k, nc)\n",
    "    W = np.random.randn(nr, k)\n",
    "    np.abs(W, W)\n",
    "    np.abs(H, H)\n",
    "    \n",
    "    I = np.ones([nr,nc])                  # unit diagonal matrix\n",
    "    for i in range(it):\n",
    "        \n",
    "        # update W\n",
    "        WH = np.dot(W,H)                  # np.dot is the matrix product\n",
    "        N = np.dot(np.divide(X,WH),H.T)   # np.divide is the element-by-element division\n",
    "        P = np.dot(I,H.T)\n",
    "        W *= np.divide(N,P)               # *= is the Hadamard product\n",
    "        \n",
    "        #update H\n",
    "        WH = np.dot(W,H)\n",
    "        N = np.dot(W.T,np.divide(X,WH))\n",
    "        P = np.dot(W.T,I)\n",
    "        H *= np.divide(N,P)\n",
    "\n",
    "    return W,H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133087e-040e-4748-b166-e373b855d56b",
   "metadata": {},
   "source": [
    "We have coded a full implementation of KL-NMF, so let's run that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e4f2a-0c07-49fd-9d9d-fde98dd52fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bases, acts = kl_nmf(mag_spec, k=n_components)\n",
    "display_components(mag_spec, bases, acts, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5184327-a23a-4cf5-8778-216f6a6b3c2b",
   "metadata": {},
   "source": [
    "There are still some differences as compared to the scikit-learn implementation, mainly due to some subtleties in the random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9438e-ca8c-4b93-bd09-6eb5a46b5c5e",
   "metadata": {},
   "source": [
    "You can now experiment with any sound, different number of components, and different cost functions!\n",
    "\n",
    "Do you identify any trend, or special behaviours? Let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a18a4-cbb3-4bdb-a8a1-8ef52ccd47b6",
   "metadata": {},
   "source": [
    "# Section 3: Wiener resynthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5fe16-c1f1-44c8-8580-045d5839d0cf",
   "metadata": {},
   "source": [
    "Until now we have decomposed and visually inspected the components. Let's make some sounds with them!\n",
    "\n",
    "To do that, we need to:\n",
    "1. Generate a time-frequency layer by modulating a spectral base with a temporal activation\n",
    "2. Compute a Wiener mask from the time-frequency layer\n",
    "3. Apply the Wiener mask to the input sound\n",
    "4. Invert the resulting spectrogram\n",
    "\n",
    "Run the following cell multiple times setting different base and activation indices (first index is 1). If act_index = base_index, you are reconstituting original components that were present in the original sound. If act_index $\\neq$ base_index, you are creating new sounds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c923a3-2d3e-4460-929e-0ef5e6accf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and display a component layer\n",
    "base_index = 1\n",
    "act_index = 1\n",
    "layer = np.outer(bases[:,base_index-1], acts[act_index-1,:])  # outer product to generate a layer\n",
    "librosa.display.specshow(librosa.amplitude_to_db(layer, ref=np.max), x_axis='time', y_axis='linear')\n",
    "plt.show()\n",
    "\n",
    "# compute the Wiener mask\n",
    "mask = np.divide(layer,np.dot(bases,acts)+0.01)  # add a small value to avoid large peaks\n",
    "\n",
    "# apply the mask to the complex input spectrogram\n",
    "filtered_spec = mask * spec\n",
    "\n",
    "# invert the spectrogram (inverse Short Time Fourier Transform)\n",
    "out_wav = librosa.istft(filtered_spec)\n",
    "\n",
    "# normalize between -1 and 1 just in case\n",
    "if np.max(out_wav) > 1.0:\n",
    "    out_wav /= np.max(np.abs(out_wav))\n",
    "\n",
    "# plot and playback the output waveform\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.plot(out_wav)\n",
    "plt.show()\n",
    "Audio(out_wav, rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b325f4a-6eb2-43fe-a3da-beb46d9b1177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
